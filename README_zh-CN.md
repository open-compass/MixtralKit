<div align="center">
  <img src="https://github.com/open-compass/MixtralKit/assets/7881589/149f8930-3a34-49b6-b27d-79dc192aeac7" width="500px"/>

  # MixtralKit
  
  Mixtral æ¨¡å‹å·¥å…·ç®±

  <a href="#-æ€§èƒ½">ğŸ“Šæ€§èƒ½ </a> â€¢
  <a href="#-ç¤¾åŒºé¡¹ç›®">âœ¨ç¤¾åŒºé¡¹ç›® </a> â€¢
  <a href="#-æ¨¡å‹æ¶æ„">ğŸ“–æ¨¡å‹æ¶æ„ </a> â€¢
  <a href="#-æ¨¡å‹æƒé‡">ğŸ“‚æ¨¡å‹æƒé‡ </a> â€¢
  <a href="#-å®‰è£…">ğŸ”¨å®‰è£… </a> â€¢
  <a href="#-æ¨ç†">ğŸš€æ¨ç† </a> â€¢
  <a href="#-è‡´è°¢">ğŸ¤è‡´è°¢ </a>

  <br />
  <br />

  [English](/README.md) | ç®€ä½“ä¸­æ–‡

</div>


> [!é‡è¦]
> <div align="center">
> <b>
> ğŸ“¢æ¬¢è¿è¯•ç”¨ <a href="https://github.com/open-compass/opencompass">OpenCompass</a> è¿›è¡Œæ¨¡å‹è¯„æµ‹ ğŸ“¢
> </b>
> <br>
> <b>
> ğŸ¤— æ¬¢è¿å°†ä½ çš„Mixtralç›¸å…³çš„é¡¹ç›®æ·»åŠ åˆ°README </a>!
> </b>
> <br>
> <b>
> ğŸ™ æœ¬ä»“åº“ä»…æä¾›**å®éªŒæ€§è´¨**çš„æ¨ç†ä»£ç 
> </b>
> </div>




# ğŸ“Š æ€§èƒ½


- æ‰€æœ‰æ•°æ®æ¥æºè‡ª[OpenCompass](https://github.com/open-compass/opencompass)

> ç”±äºä¸åŒè¯„æµ‹æ¡†æ¶åœ¨æç¤ºè¯ï¼Œè¯„æµ‹è®¾å®šå’Œå®ç°ç»†èŠ‚ä¸Šå‡æœ‰æ‰€ä¸åŒï¼Œæ‰€ä»¥è¯·å‹¿ç›´æ¥å¯¹æ¯”ä¸åŒæ¡†æ¶è·å¾—çš„è¯„æµ‹ç»“æœã€‚

## æ€§èƒ½å¯¹æ¯”


| æ•°æ®é›†        | Mode | Mistral-7B-v0.1 | Mixtral-8x7B(MoE) |  Llama2-70B | DeepSeek-67B-Base | Qwen-72B | 
|-----------------|------|-----------------|--------------|-------------|-------------------|----------|
| æ¿€æ´»å‚æ•°   |  -   |      7B         |     12B      |     70B     |       67B         |   72B    |
| MMLU            | PPL  | 64.1            | 71.3         | 69.7        | 71.9              | 77.3     |
| BIG-Bench-Hard  | GEN  | 56.7            | 67.1         | 64.9        | 71.7              | 63.7     |
| GSM-8K          | GEN  | 47.5            | 65.7         | 63.4        | 66.5              | 77.6     |
| MATH            | GEN  | 11.3            | 22.7         | 12.0        | 15.9              | 35.1     |
| HumanEval       | GEN  | 27.4            | 32.3         | 26.2        | 40.9              | 33.5     |
| MBPP            | GEN  | 38.6            | 47.8         | 39.6        | 55.2              | 51.6     |
| ARC-c           | PPL  | 74.2            | 85.1         | 78.3        | 86.8              | 92.2     |
| ARC-e           | PPL  | 83.6            | 91.4         | 85.9        | 93.7              | 96.8     |
| CommonSenseQA   | PPL  | 67.4            | 70.4         | 78.3        | 70.7              | 73.9     |
| NaturalQuestion | GEN  | 24.6            | 29.4         | 34.2        | 29.9              | 27.1     |
| TrivialQA       | GEN  | 56.5            | 66.1         | 70.7        | 67.4              | 60.1     |
| HellaSwag       | PPL  | 78.9            | 82.0         | 82.3        | 82.3              | 85.4     |
| PIQA            | PPL  | 81.6            | 82.9         | 82.5        | 82.6              | 85.2     |
| SIQA            | GEN  | 60.2            | 64.3         | 64.8        | 62.6              | 78.2     |


## Mixtral-8x7b æ€§èƒ½

```markdown
dataset                                 version    metric         mode    mixtral-8x7b-32k
--------------------------------------  ---------  -------------  ------  ------------------
mmlu                                    -          naive_average     ppl     71.34
ARC-c                                   2ef631     accuracy          ppl     85.08
ARC-e                                   2ef631     accuracy          ppl     91.36
BoolQ                                   314797     accuracy          ppl     86.27
commonsense_qa                          5545e2     accuracy          ppl     70.43
triviaqa                                2121ce     score             gen     66.05
nq                                      2121ce     score             gen     29.36
openbookqa_fact                         6aac9e     accuracy          ppl     85.40
AX_b                                    6db806     accuracy          ppl     48.28
AX_g                                    66caf3     accuracy          ppl     48.60
hellaswag                               a6e128     accuracy          ppl     82.01
piqa                                    0cfff2     accuracy          ppl     82.86
siqa                                    e8d8c5     accuracy          ppl     64.28
math                                    265cce     accuracy          gen     22.74
gsm8k                                   1d7fe4     accuracy          gen     65.66
openai_humaneval                        a82cae     humaneval_pass@1  gen     32.32
mbpp                                    1e1056     score             gen     47.80
bbh                                     -          naive_average     gen     67.14
```
# âœ¨ ç¤¾åŒºé¡¹ç›®

## åšå®¢
- [MoE Blog from HuggingFace](https://huggingface.co/blog/moe)
- [Enhanced MoE Parallelism, Open-source MoE Model Training Can Be 9 Times More Efficient](https://www.hpc-ai.tech/blog/enhanced-moe-parallelism-open-source-moe-model-training-can-be-9-times-more-efficient)

## è®ºæ–‡

|  é¢˜ç›®  |   ä¼šè®®/æœŸåˆŠ  |   æ—¥æœŸ   |   ä»£ç    |   ç¤ºä¾‹   |
|:--------|:--------:|:--------:|:--------:|:--------:|
|[Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models](https://arxiv.org/abs/2305.14705)           | Arxiv       | 23.05 | | 
|[MegaBlocks: Efficient Sparse Training with Mixture-of-Experts](https://arxiv.org/abs/2211.15841)                                         | Arxiv       | 22.11 | [megablocks](https://github.com/stanford-futuredata/megablocks) | |
|[ST-MoE: Designing Stable and Transferable Sparse Expert Models](https://arxiv.org/abs/2202.08906)                                        | Arxiv       | 22.02 |
|[Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/abs/2101.03961)          | Arxiv       | 21.01 |
|[GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/abs/2112.06905)                                    | ICML 2022   | 21.12 |
|[GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding](https://arxiv.org/abs/2006.16668)                      | Arxiv       | 20.06 |
|[Learning Factored Representations in a Deep Mixture of Experts](https://arxiv.org/abs/1312.4314)                                         | Arxiv       | 13.12 |
|[FastMoE: A Fast Mixture-of-Expert Training System](https://arxiv.org/abs/2103.13262)   | Arxiv | 21.03 | [FastMoE](https://github.com/laekov/FastMoE)|
|[FasterMoE: Modeling and Optimizing Training of Large-scale Dynamic Pre-trained Models](https://dl.acm.org/doi/10.1145/3503221.3508418)   | ACM SIGPLAN PPoPP 2022 | 22.03 | [FasterMoE](https://github.com/laekov/FastMoE)|
|[SmartMoE: Efficiently Training Sparsely-Activated Models through Combining Offline and Online Parallelization](https://www.usenix.org/conference/atc23/presentation/zhai)   | USENIX ATC 2023 | 22.03 | [SmartMoE](https://github.com/zms1999/SmartMoE)|
|[Adaptive Mixture of Local Experts](https://www.cs.toronto.edu/~hinton/absps/jjnh91.pdf)                                                  | Neural Computation | 1991 |

## è¯„æµ‹
- [x] è¯„æµ‹å·¥å…· [OpenCompass](https://github.com/open-compass/opencompass)

## è®­ç»ƒ
- Megablocks: https://github.com/stanford-futuredata/megablocks
- FairSeq: https://github.com/facebookresearch/fairseq/tree/main/examples/moe_lm
- OpenMoE: https://github.com/XueFuzhao/OpenMoE
- ColossalAI MoE: https://github.com/hpcaitech/ColossalAI/tree/main/examples/language/openmoe
- FastMoE(FasterMoE): https://github.com/laekov/FastMoE
- SmartMoE: https://github.com/zms1999/SmartMoE

## å¾®è°ƒ
- [x] ä½¿ç”¨XTunerå¾®è°ƒMixtral-8x7B æ–¹æ¡ˆ(å…¨å‚æ•°/QLoRA): [XTuner](https://github.com/InternLM/xtuner/tree/main/xtuner/configs/mixtral) 
- [x] å¾®è°ƒæ¨¡å‹Mixtral-8x7B(DiscoResearch): [DiscoLM-mixtral-8x7b-v2](https://huggingface.co/DiscoResearch/DiscoLM-mixtral-8x7b-v2)

## éƒ¨ç½²

TBD

# ğŸ“– æ¨¡å‹æ¶æ„

> Mixtral-8x7B-32K MoEæ¨¡å‹ä¸»è¦ç”±32ä¸ªç›¸åŒçš„MoEtransformer blockç»„æˆã€‚MoEtransformer blockä¸æ™®é€šçš„transformer blockçš„æœ€å¤§å·®åˆ«åœ¨äºå…¶FFNå±‚æ›¿æ¢ä¸ºäº†**MoE FFN**å±‚ã€‚åœ¨MoE FFNå±‚ï¼Œtensoré¦–å…ˆä¼šç»è¿‡ä¸€ä¸ªgate layerè®¡ç®—æ¯ä¸ªexpertçš„å¾—åˆ†ï¼Œå¹¶æ ¹æ®expertå¾—åˆ†ä»8ä¸ªexpertä¸­æŒ‘å‡ºtop-kä¸ªexpertï¼Œå°†tensorç»è¿‡è¿™top-kä¸ªexpertçš„è¾“å‡ºåèšåˆèµ·æ¥ï¼Œä»è€Œå¾—åˆ°MoE FFNå±‚çš„æœ€ç»ˆè¾“å‡ºï¼Œå…¶ä¸­çš„æ¯ä¸ªexpertç”±3ä¸ªLinearå±‚ç»„æˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œmixtral MoEçš„æ‰€æœ‰Norm Layerä¹Ÿé‡‡ç”¨äº†å’ŒLLamaä¸€æ ·çš„RMSNormï¼Œè€Œåœ¨attention layerä¸­ï¼Œmixtral MoEçš„QKVçŸ©é˜µä¸­çš„QçŸ©é˜µshapeä¸º(4096,4096)ï¼ŒKå’ŒVçŸ©é˜µshapeåˆ™ä¸º(4096,1024)ã€‚

æ¨¡å‹ç»“æ„å›¾å¦‚ä¸‹:

<div align="center">
  <img src="https://github.com/open-compass/MixtralKit/assets/7881589/0bd59661-4799-4e39-8a92-95fd559679e9" width="800px"/>
</div>


# ğŸ“‚ æ¨¡å‹æƒé‡

## HuggingFace æ ¼å¼

- [å®˜æ–¹åŸºåº§æ¨¡å‹ Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)
- [å®˜æ–¹å¯¹è¯æ¨¡å‹ Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)

## åŸç”Ÿæ ¼å¼

ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨ç£åŠ›é“¾æ¥(è¿…é›·)æˆ–ä½¿ç”¨HuggingFaceè¿›è¡Œä¸‹è½½

### ä½¿ç”¨HFä¸‹è½½

ç¤¾åŒºç”¨æˆ·æä¾›çš„HFæ–‡ä»¶åˆ‡åˆ†ç‰ˆï¼š[HuggingFaceä»“åº“](https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen)

```bash
# Download the huggingface
git lfs install
git clone https://huggingface.co/someone13574/mixtral-8x7b-32kseqlen

```
> ç”¨æˆ·å¦‚æœæ— æ³•è®¿é—®huggingface, å¯ä»¥ä½¿ç”¨[å›½å†…é•œåƒ](https://hf-mirror.com/someone13574/mixtral-8x7b-32kseqlen)

```bash
# Download the huggingface
git lfs install
git clone https://hf-mirror.com/someone13574/mixtral-8x7b-32kseqlen

# Merge Files(Only for HF)
cd mixtral-8x7b-32kseqlen/

# Merge the checkpoints
cat consolidated.00.pth-split0 consolidated.00.pth-split1 consolidated.00.pth-split2 consolidated.00.pth-split3 consolidated.00.pth-split4 consolidated.00.pth-split5 consolidated.00.pth-split6 consolidated.00.pth-split7 consolidated.00.pth-split8 consolidated.00.pth-split9 consolidated.00.pth-split10 > consolidated.00.pth
```

### ä½¿ç”¨ç£åŠ›é“¾ä¸‹è½½

```bash
magnet:?xt=urn:btih:5546272da9065eddeb6fcd7ffddeef5b75be79a7&dn=mixtral-8x7b-32kseqlen&tr=udp%3A%2F%http://2Fopentracker.i2p.rocks%3A6969%2Fannounce&tr=http%3A%2F%http://2Ftracker.openbittorrent.com%3A80%2Fannounce
```


### æ–‡ä»¶æ ¡éªŒ

è¯·åœ¨ä½¿ç”¨æ–‡ä»¶å‰ï¼Œè¿›è¡Œmd5æ ¡éªŒï¼Œä¿è¯æ–‡ä»¶åœ¨ä¸‹è½½è¿‡ç¨‹ä¸­å¹¶æœªæŸå
```bash
md5sum consolidated.00.pth
md5sum tokenizer.model

# å¦‚æœå®Œæˆæ ¡éªŒï¼Œå¯åˆ é™¤slitæ–‡ä»¶
rm consolidated.00.pth-split*
```

å®˜æ–¹æ ¡éªŒå€¼

```bash
 â•“â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•–
 â•‘                                                                            â•‘
 â•‘                               Â·Â· md5sum Â·Â·                                 â•‘
 â•‘                                                                            â•‘
 â•‘        1faa9bc9b20fcfe81fcd4eb7166a79e6  consolidated.00.pth               â•‘
 â•‘        37974873eb68a7ab30c4912fc36264ae  tokenizer.model                   â•‘
 â•™â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•œ
```

# ğŸ”¨ å®‰è£…

```bash
git clone https://github.com/open-compass/MixtralKit
cd MixtralKit/
pip install -r requirements.txt
pip install -e .

ln -s path/to/checkpoints ckpts
```

# ğŸš€ æ¨ç†

## æ–‡æœ¬è¡¥å…¨

```bash
==============================Example START==============================

[Prompt]:
Who are you?

[Response]:
I am a designer and theorist; a lecturer at the University of Malta and a partner in the firm Barbagallo and Baressi Design, which won the prestig
ious Compasso dâ€™Oro award in 2004. I was educated in industrial and interior design in the United States

==============================Example END==============================

==============================Example START==============================

[Prompt]:
1 + 1 -> 3
2 + 2 -> 5
3 + 3 -> 7
4 + 4 ->

[Response]:
9
5 + 5 -> 11
6 + 6 -> 13

#include <iostream>

using namespace std;

int addNumbers(int x, int y)
{
        return x + y;
}

int main()
{

==============================Example END==============================

```


# ğŸ—ï¸ è¯„æµ‹

## ç¬¬ä¸€æ­¥: é…ç½®OpenCompass

- å…‹éš†å’Œå®‰è£… OpenCompass

```bash
# assume you have already create the conda env named mixtralkit 
conda activate mixtralkit

git clone https://github.com/open-compass/opencompass opencompass
cd opencompass

pip install -e .
```

- å‡†å¤‡è¯„æµ‹æ•°æ®é›†

```bash
# Download dataset to data/ folder
wget https://github.com/open-compass/opencompass/releases/download/0.1.8.rc1/OpenCompassData-core-20231110.zip
unzip OpenCompassData-core-20231110.zip
```

> If you need to evaluate the **humaneval**, please go to [Installation Guide](https://opencompass.readthedocs.io/en/latest/get_started/installation.html) for more information


## ç¬¬äºŒæ­¥: å‡†å¤‡è¯„æµ‹é…ç½®æ–‡ä»¶å’Œæ•°æ®é›†

```bash
cd opencompass/
# link the example config into opencompass
ln -s path/to/MixtralKit/playground playground

# link the model weights into opencompass
mkdir -p ./models/mixtral/
ln -s path/to/checkpoints_folder/ ./models/mixtral/mixtral-8x7b-32kseqlen
```

ç°åœ¨æ–‡ä»¶ç»“æ„åº”è¯¥å¦‚ä¸‹æ‰€ç¤º

```bash
opencompass/
â”œâ”€â”€ configs
â”‚Â Â  â”œâ”€â”€ .....
â”‚Â Â  â””â”€â”€ .....
â”œâ”€â”€ models
â”‚Â Â  â””â”€â”€ mixtral
â”‚Â Â      â””â”€â”€ mixtral-8x7b-32kseqlen
â”œâ”€â”€ data/
â”œâ”€â”€ playground
â”‚Â Â  â””â”€â”€ eval_mixtral.py
â”‚â”€â”€ ......
```


## ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œè¯„æµ‹

```bash
HF_EVALUATE_OFFLINE=1 HF_DATASETS_OFFLINE=1 TRANSFORMERS_OFFLINE=1 python run.py playground/eval_mixtral.py

# è¯·ç¼–è¾‘playground/eval_mixtral.pyæ¥é…ç½®å¸Œæœ›è¯„æµ‹çš„æ•°æ®é›†

```

# ğŸ¤ è‡´è°¢
- [llama-mistral](https://github.com/dzhulgakov/llama-mistral)
- [llama](https://github.com/facebookresearch/llama)

# ğŸ–Šï¸ å¼•ç”¨


```latex
@misc{2023opencompass,
    title={OpenCompass: A Universal Evaluation Platform for Foundation Models},
    author={OpenCompass Contributors},
    howpublished = {\url{https://github.com/open-compass/opencompass}},
    year={2023}
}
```
